1-# Simulate or Collect Sensor Data: Use an MQTT broker to simulate real-time sensor data.
2-# Real-time Data Ingestion: Use MQTT to publish and subscribe to the sensor data.
3-# Data Processing: Process the real-time data (e.g., filter out bad data, clean up).
4-# AI Model Inference: Send the processed data to a TensorFlow model for prediction.
........ Tools and Libraries Required
# paho-mqtt: For communication between sensors (publisher) and the processing system (subscriber).
# TensorFlow or TensorFlow-Serving: To deploy the AI model and run real-time inference.
# Jupyter Notebook (inside GitHub Codespaces): For development and testing.

1- # Simulate or Collect Sensor Data: Use an MQTT broker to simulate real-time sensor data.

!pip install paho-mqtt
import random
import time
import paho.mqtt.client as mqtt  # MQTT library for sending data

# MQTT Broker settings
BROKER = "broker.hivemq.com"  # Public MQTT broker
PORT = 1883
TOPIC = "sensor/temperature"

# Function to simulate sensor data and publish to MQTT
def simulate_sensor_data():
    client = mqtt.Client()
    client.connect(BROKER, PORT)

    while True:
        # Simulate random temperature data
        temperature = round(random.uniform(20.0, 30.0), 2) # random temperature readings between 20°C and 30°C and publishes them every 2 seconds to the MQTT topic sensor/temperature on the broker.hivemq.com server.
        client.publish(TOPIC, f"{temperature}")
        print(f"Published: {temperature}°C to topic {TOPIC}")
        time.sleep(2)  # Send data every 2 seconds

if __name__ == "__main__":
    simulate_sensor_data()

2 # Ingest Real-Time Data Using MQTT
# This code listens for sensor data published to the topic sensor/temperature and receives the temperature data in real time.

import paho.mqtt.client as mqtt

# MQTT Broker settings
BROKER = "broker.hivemq.com"
PORT = 1883
TOPIC = "sensor/temperature"

# Callback function that gets executed when the client receives a message
def on_message(client, userdata, message):
    temperature = message.payload.decode()  # Decode the received message
    print(f"Received temperature: {temperature}°C from topic {message.topic}")

# Callback function to confirm connection to the broker
def on_connect(client, userdata, flags, rc):
    if rc == 0:
        print("Connected to MQTT Broker!")
    else:
        print(f"Failed to connect, return code {rc}")

# Set up the MQTT client
client = mqtt.Client()

# Attach the callback functions
client.on_connect = on_connect
client.on_message = on_message

# Connect to the MQTT broker
client.connect(BROKER, PORT)

# Subscribe to the sensor/temperature topic
client.subscribe(TOPIC)

# Start listening for incoming messages
print(f"Subscribed to {TOPIC}. Waiting for messages...")
client.loop_forever()  # This keeps the connection and waits for messages

3. # Data Processing: Process the real-time data (e.g., filter out bad data, clean up).
# to process the incoming data.  example, we'll ignore temperature readings below 22°C and above 28°C as potential outliers.

import paho.mqtt.client as mqtt

# MQTT Broker settings
BROKER = "broker.hivemq.com"
PORT = 1883
TOPIC = "sensor/temperature"

# Function to process data (simple outlier detection)
def process_data(temperature):
    if 22 <= temperature <= 28:
        print(f"Processed temperature: {temperature}°C")
        return True  # Only process valid data
    else:
        print(f"Ignoring outlier temperature: {temperature}°C")
        return False

# Callback function when a message is received
def on_message(client, userdata, message):
    temperature = float(message.payload.decode())  # Decode and convert to float
    print(f"Received temperature: {temperature}°C from topic {message.topic}")
    
    # Process the data
    if process_data(temperature):
        # Send to AI Model (next step)
        send_to_ai_model(temperature)

# Callback for connection confirmation
def on_connect(client, userdata, flags, rc):
    if rc == 0:
        print("Connected to MQTT Broker!")
    else:
        print(f"Failed to connect, return code {rc}")

# Set up the MQTT client
client = mqtt.Client()

# Attach the callback functions
client.on_connect = on_connect
client.on_message = on_message

# Connect to the MQTT broker
client.connect(BROKER, PORT)

# Subscribe to the sensor/temperature topic
client.subscribe(TOPIC)

# Start listening for messages
print(f"Subscribed to {TOPIC}. Waiting for messages...")
client.loop_forever()

4. # AI Model Inference: Send the processed data to a TensorFlow model for prediction.
# send the processed data to an AI model for inference
# pre-trained TensorFlow model
# API endpoint to make predictions
# Install TensorFlow
# create and save the Model
import tensorflow as tf
from tensorflow.keras import layers, models

# Define a simple sequential model
def create_model():
    model = models.Sequential([
        layers.Input(shape=(1,)),  # Input is a single feature (e.g., temperature)
        layers.Dense(64, activation='relu'),  # Dense layer with 64 units
        layers.Dense(1)  # Output layer with a single unit (e.g., regression output)
    ])
    model.compile(optimizer='adam', loss='mean_squared_error')
    return model

# Create the model
model = create_model()

# Optional: Train the model on some sample data
import numpy as np
X_train = np.array([[20.5], [22.1], [23.3], [24.8], [25.6], [27.1], [28.2], [29.3]])
y_train = np.array([[31.0], [32.5], [34.0], [35.8], [36.5], [39.2], [40.1], [41.3]])

# Train the model (optional)
model.fit(X_train, y_train, epochs=10)

# Save the model in the SavedModel format
model.save('my_model')  # This will create a directory called "my_model"
print("Model saved successfully.")

# pre-trained Modle
import tensorflow as tf
import tensorflow_hub as hub

# Load a pre-trained model from TensorFlow Hub (example: MobileNet for image classification)
model = tf.keras.Sequential([
    hub.KerasLayer("https://tfhub.dev/google/tf2-preview/mobilenet_v2/classification/4")
])

# Save the model so you can serve it
model.save('my_pretrained_model')

# Testing the Full Pipeline with TensorFlow Serving

import paho.mqtt.client as mqtt
import requests
import json

# MQTT Broker settings
BROKER = "broker.hivemq.com"
PORT = 1883
TOPIC = "sensor/temperature"

# AI Model API (TensorFlow Serving)
MODEL_API_URL = "http://localhost:8501/v1/models/my_model:predict"

# Function to process data and send it to the TensorFlow model
def send_to_ai_model(temperature):
    payload = {
        "instances": [{"input_1": [temperature]}]  # TensorFlow Serving expects this format
    }
    headers = {"content-type": "application/json"}
    response = requests.post(MODEL_API_URL, data=json.dumps(payload), headers=headers)
    
    if response.status_code == 200:
        prediction = response.json()["predictions"]
        print(f"AI Model Prediction: {prediction}")
    else:
        print(f"Failed to get prediction. Status code: {response.status_code}")

# Callback function when a message is received
def on_message(client, userdata, message):
    temperature = float(message.payload.decode())  # Decode the message
    print(f"Received temperature: {temperature}°C from topic {message.topic}")
    
    # Send the data to the AI model for prediction
    send_to_ai_model(temperature)

# Set up the MQTT client
client = mqtt.Client()

# Set the callback functions
client.on_message = on_message

# Connect to the MQTT broker
client.connect(BROKER, PORT)

# Subscribe to the topic
client.subscribe(TOPIC)

# Start the client loop to listen for messages
client.loop_forever()

# Testing the Full Pipeline with TensorFlow Serving

import paho.mqtt.client as mqtt
import requests
import json

# MQTT Broker settings
BROKER = "broker.hivemq.com"
PORT = 1883
TOPIC = "sensor/temperature"

# AI Model API (TensorFlow Serving)
MODEL_API_URL = "http://localhost:8501/v1/models/my_model:predict"

# Function to process data and send it to the TensorFlow model
def send_to_ai_model(temperature):
    payload = {
        "instances": [{"input_1": [temperature]}]  # TensorFlow Serving expects this format
    }
    headers = {"content-type": "application/json"}
    response = requests.post(MODEL_API_URL, data=json.dumps(payload), headers=headers)
    
    if response.status_code == 200:
        prediction = response.json()["predictions"]
        print(f"AI Model Prediction: {prediction}")
    else:
        print(f"Failed to get prediction. Status code: {response.status_code}")

# Callback function when a message is received
def on_message(client, userdata, message):
    temperature = float(message.payload.decode())  # Decode the message
    print(f"Received temperature: {temperature}°C from topic {message.topic}")
    
    # Send the data to the AI model for prediction
    send_to_ai_model(temperature)

# Set up the MQTT client
client = mqtt.Client()

# Set the callback functions
client.on_message = on_message

# Connect to the MQTT broker
client.connect(BROKER, PORT)

# Subscribe to the topic
client.subscribe(TOPIC)

# Start the client loop to listen for messages
client.loop_forever()



# or Testing the Full Pipeline with Mock API
import paho.mqtt.client as mqtt
import requests
import json

# MQTT Broker settings
BROKER = "broker.hivemq.com"
PORT = 1883
TOPIC = "sensor/temperature"

# AI Model Mock API URL
MODEL_API_URL = "http://localhost:8501/v1/models/mock_model:predict"

# Function to process data and send it to the mock AI model
def send_to_ai_model(temperature):
    payload = {
        "instances": [{"temperature": temperature}]
    }
    headers = {"content-type": "application/json"}
    response = requests.post(MODEL_API_URL, data=json.dumps(payload), headers=headers)
    
    if response.status_code == 200:
        prediction = response.json()["predictions"]
        print(f"AI Model Prediction: {prediction}")
    else:
        print(f"Failed to get prediction. Status code: {response.status_code}")

# Callback function when a message is received from MQTT
def on_message(client, userdata, message):
    temperature = float(message.payload.decode())  # Decode the message
    print(f"Received temperature: {temperature}°C from topic {message.topic}")
    
    # Send the data to the mock AI model for prediction
    send_to_ai_model(temperature)

# Set up the MQTT client
client = mqtt.Client()

# Set the callback functions
client.on_message = on_message

# Connect to the MQTT broker
client.connect(BROKER, PORT)

# Subscribe to the topic
client.subscribe(TOPIC)

# Start the client loop to listen for messages
client.loop_forever()
